import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()

        assert d_out % num_heads == 0, "d_out must be divisible by num_heads"
        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # head_dim = d_out / num_heads

        # Linear projections for queries, keys, and values
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

        # Final output projection layer
        self.out_proj = nn.Linear(d_out, d_out)

        # Dropout
        self.dropout = nn.Dropout(dropout)

        # Causal mask: Upper triangular matrix with 1s above the diagonal
        self.register_buffer("mask", torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        # Step 1: Linear projections
        keys    = self.W_key(x)     # shape: [b, num_tokens, d_out]
        queries = self.W_query(x)
        values  = self.W_value(x)

        # Step 2: Split into multiple heads
        keys    = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)
        values  = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)
        # Shape for all: [b, num_heads, num_tokens, head_dim]

        # Step 3: Attention scores
        attn_scores = queries @ keys.transpose(2, 3)  # [b, heads, tokens, tokens]

        # Step 4: Apply causal mask
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]
        attn_scores = attn_scores.masked_fill(mask_bool, float('-inf'))

        # Step 5: Softmax + dropout
        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Step 6: Compute attention output
        context = attn_weights @ values  # [b, heads, tokens, head_dim]
        context = context.transpose(1, 2).contiguous()  # [b, tokens, heads, head_dim]

        # Step 7: Merge heads and final projection
        context = context.view(b, num_tokens, self.d_out)  # [b, tokens, d_out]
        output = self.out_proj(context)  # Final linear projection

        return output
# Create dummy input
batch = torch.randn(1, 3, 6)  # [batch_size=1, num_tokens=3, d_in=6]

# Initialize the MultiHeadAttention module
torch.manual_seed(123)
mha = MultiHeadAttention(d_in=6, d_out=6, context_length=3, dropout=0.0, num_heads=2)

# Forward pass
output = mha(batch)
print("Output:\n", output)
print("Shape:", output.shape)  # Expected: [1, 3, 6]
