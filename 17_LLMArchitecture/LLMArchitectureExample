import torch
import torch.nn as nn

# Configuration dictionary for model hyperparameters
GPT_CONFIG_124M = {
    "vocab_size": 50257,
    "context_length": 1024,
    "emb_dim": 768,
    "n_heads": 12,
    "n_layers": 12,
    "drop_rate": 0.1,
    "qkv_bias": False
}

# Step 1: Token and Positional Embedding Layers
tok_emb = nn.Embedding(GPT_CONFIG_124M["vocab_size"], GPT_CONFIG_124M["emb_dim"])
pos_emb = nn.Embedding(GPT_CONFIG_124M["context_length"], GPT_CONFIG_124M["emb_dim"])
drop_emb = nn.Dropout(GPT_CONFIG_124M["drop_rate"])
# Input batch of token indices
batch = torch.tensor([
    [6109, 3626, 6100, 345],   # "Every effort moves you"
    [6109, 1110, 6622, 257]    # "Every day holds a"
])

batch_size, seq_len = batch.shape

# Token embeddings: [2, 4, 768]
tok_embeds = tok_emb(batch)

# Positional indices: [0, 1, 2, 3]
pos_ids = torch.arange(seq_len, device=batch.device)

# Positional embeddings: [4, 768] → [1, 4, 768] → broadcast to [2, 4, 768]
pos_embeds = pos_emb(pos_ids)

# Combine both: [2, 4, 768]
x = tok_embeds + pos_embeds

# Apply dropout
x = drop_emb(x)

print("Embeddings shape:", x.shape)

# Placeholder Transformer Block - we'll replace this later
class DummyTransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
    def forward(self, x):
        return x  # Just passes input forward for now
# Stack multiple transformer blocks (12 in GPT-2 small)
trf_blocks = nn.Sequential(
    *[DummyTransformerBlock(GPT_CONFIG_124M) for _ in range(GPT_CONFIG_124M["n_layers"])]
)

# Apply them
x = trf_blocks(x)

# Placeholder LayerNorm
class DummyLayerNorm(nn.Module):
    def __init__(self, normalized_shape, eps=1e-5):
        super().__init__()
    def forward(self, x):
        return x

# Final steps
final_norm = DummyLayerNorm(GPT_CONFIG_124M["emb_dim"])
x = final_norm(x)

# Output head: project to vocabulary size
out_head = nn.Linear(GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["vocab_size"], bias=False)
logits = out_head(x)

print("Logits shape:", logits.shape)
